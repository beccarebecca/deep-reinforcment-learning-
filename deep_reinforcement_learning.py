# -*- coding: utf-8 -*-
"""deep reinforcement learning

Automatically generated by Colaboratory.
by Rebecca N """
!pip install gym

import gym
import numpy as np
import random
from keras.layers import Dense, Activation
from keras.models import Model, Sequential
from keras import optimizers
import random
memList = []
degradePicker = 1
episodes = 600
craniumFull = 2000
craniumDegrade = .0099

#neuralnetwork class
class superNet:
  def __init__(self, stateSize, actionSize):
    self.actionSize = actionSize
    self.stateSize = statesize
    self.learnR = 0.005
    self.batchSize = 20
    self.discount = .0089
# basic neural network
  def nn(self):
    model = Sequential()
    model.add(Dense(160, input_dim = self.stateSize,  activation = 'LeakyReLU(alpha = 0.3)'))
    model.add(Dense(160, activation = 'LeakyReLU(alpha = 0.3)'))
    model.add(Dense(80, activation = 'LeakyReLU(alpha = 0.3)'))
    model.add(Dense(40, activation = 'LeakyReLU(alpha = 0.3)'))
    model.add(Dense(self.actionSize, activation = 'linear'))
    model.compile( loss = 'mse' , optimizer = Adam(lr = learnR))
    return model
  
  def train(self):
  for state,action, reward, nextState , done in randomPicks(batchSize = self.batchSize):
  target = reward + discount * np.argmax(targetNN.predict(nextState)) 
  model.fit(state, target,epoch = 1, verbose = 0 , * self.learnR)
  if frame == 50:
      targetNN.model.fit(state, target,epoch =1, verbose = 0, * self.learnR)
  
  #train on random samples from memList
  def randomPicks(self, memList = memList, batchSize):
    return random.sample(memList, batchSize) if batchSize > 14 else random.sample(memList, len(memList))



# storing states, next sate, reward for training later on
 def libraryC(state, action,  reward,next_state,done):
   if len(memList) < craniumFull:
    memList.append([state, reward, next_state, done])
   else:
      memList.slice[1:]
   craniumFull *= craniumDegrade if craniumDegrade > .001 else craniumFull
   return memList

#get next action/step
def actionPick(state):
  return np.argmax(nn.predict(state)) if random.random() < degradePicker else random.randrange(2)

#set up gym enviroment
env = gym.make("CartPole-v0")
nnMan = superNet(env.state_size, env.action_size)
targetNN = superNet(env)
for episode in episodes:
  state = env.reset()
  state = np.reshape(state, [1,4])
  
  for frame in 1000:
    state.render()
    step = actionPick(state)
    np.reshape(nextState, [1,4]) , reward, done, _ = env.step(step)
    libraryC(state, step, reward, nextState, done)
    rant = random.randrange(memlist)
    #train model
    
    #evaluate model
    nnMan.train()
    model.evaluate(memlist[rant][0], reward + discount * np.argmax(targetNN.predict(memList[rant][3])), verbose = 1)
    
    state = nextState
   
    if done:
      print("number or rounds is {} out of  {} with the score of  {}".format(episode,episodes, frame) )